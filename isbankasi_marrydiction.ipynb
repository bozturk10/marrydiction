{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "isbankasi_marrydiction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdBKk2P0V0v5"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install catboost\n",
        "!pip install statsmodels==0.12.1\n",
        "from catboost import CatBoostClassifier, Pool, cv\n",
        "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
        "from sklearn.metrics import fbeta_score,confusion_matrix,accuracy_score,f1_score,classification_report\n",
        "pd.set_option(\"display.max_columns\",None)\n",
        "pd.set_option('display.max_rows', 150)\n",
        "sns.set_theme(style=\"darkgrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRaP_LOJWFki"
      },
      "source": [
        "cd drive/MyDrive/IsBankasi/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5fEZWKyrvYB"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "*train.csv:* \n",
        "  - customer info (id,age,customer seniority, job, etc.) and target \n",
        "\n",
        "*monthly_expenditures.csv:*\n",
        "  - cid, transaction type, transaction amount(aggregated for month), date(month), quantity.\n",
        "\n",
        "target:\n",
        " - whether the customer has married in the next 6 months (last 6 months of year) of their expenditure records.(first 6 months of year.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npNK67WCWNYC",
        "outputId": "6694ea90-239d-4593-8854-7bdb1779d943"
      },
      "source": [
        "df_train=pd.read_csv(\"train.csv\").drop(columns=\"tarih\").fillna(\"nan\")\n",
        "df_test=pd.read_csv(\"test.csv\").drop(columns=\"tarih\").fillna(\"nan\")\n",
        "df_all=pd.concat([df_train,df_test],axis=0)\n",
        "df_exp=pd.read_csv(\"monthly_expenditures.csv\")\n",
        "df_exp.tarih=pd.to_datetime(df_exp.tarih.astype(str),format='%Y%m%d').dt.month\n",
        "#df_exp[\"ge10K\"]=(df_exp.aylik_toplam_tutar > 10000).astype(int)\n",
        "print(\"train: \",df_train.shape)\n",
        "print(\"test: \",df_test.shape)\n",
        "print(\"exp:\",df_exp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:  (60000, 7)\n",
            "test:  (40000, 6)\n",
            "exp: (932144, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSIdVVTgtfFC"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "1) marking the marginal transactions\n",
        "- grouping the users with respect to demographics\n",
        "- marking the marginal transactions (5%) for each demographic group and transaction category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVy_fbgOwwPP"
      },
      "source": [
        "profile_data=pd.merge(df_exp,df_all,on=\"musteri\").drop(columns=\"target\").groupby(['egitim',\"is_durumu\",'meslek_grubu','sektor']).quantile(.95).reset_index()[[\"egitim\",\t\"is_durumu\",\t\"meslek_grubu\",\t\"sektor\",\t\"aylik_toplam_tutar\"]]\n",
        "profile_data.columns=[\"egitim\",\t\"is_durumu\",\t\"meslek_grubu\",\t\"sektor\",\"group_aylik_toplam_tutar_95_perc\"]\n",
        "df_exp_with_personal_info=pd.merge(df_exp,df_all,on=\"musteri\",how=\"left\").drop(columns=[\"yas\",\"target\",\"kidem_suresi\"])\n",
        "df_exp_extended=pd.merge(df_exp_with_personal_info,profile_data,on=[\"egitim\",\"is_durumu\",\"meslek_grubu\",\"sektor\"],how=\"left\")\n",
        "df_exp_extended[\"is_outlier_transaction\"]= (df_exp_extended.group_aylik_toplam_tutar_95_perc < df_exp_extended.aylik_toplam_tutar).astype(int)\n",
        "df_exp_extended= df_exp_extended.drop(columns=[\"egitim\"\t,\"is_durumu\",\t\"meslek_grubu\"\t,\"group_aylik_toplam_tutar_95_perc\"])\n",
        "df_exp=df_exp_extended"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ22HEzVGH_V"
      },
      "source": [
        "\n",
        "def lin_regression(df):\n",
        "  try:\n",
        "    y=df['aylik_toplam_tutar'].values\n",
        "    x=df['tarih'].values\n",
        "    if (len(y)==1):\n",
        "      slope= np.nan\n",
        "    else:  \n",
        "      m_x=np.mean(x)\n",
        "      m_y=np.mean(y)\n",
        "      slope=np.sum((x-m_x)*(y-m_y)) / np.sum((x-m_x)**2)\n",
        "  except:\n",
        "        slope = np.nan\n",
        "\n",
        "  return slope"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBMIZW-6u011"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmpIu3b9u1I2"
      },
      "source": [
        "2) Aggregate Variables for each customer (all sectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0mMjgVWadQv"
      },
      "source": [
        "agg_vars_musteri_level= df_exp.groupby(['musteri']).agg({'islem_adedi':['mean','std','sum','max'],\n",
        "                                                           'aylik_toplam_tutar':['mean','std','sum','max','min'],\n",
        "                                                         }).reset_index()\n",
        "agg_vars_musteri_level.columns = [\"_\".join(x) if len(x[1]) > 1 else x[0] for x in agg_vars_musteri_level.columns.ravel()]\n",
        "agg_vars_musteri_level_cols=[x  for x in agg_vars_musteri_level.columns if \"_\" in x]\n",
        "\n",
        "#calculating the trend slope of total amount of transactions\n",
        "harcama_trend_tekil=df_exp.groupby(by=[\"musteri\",\"tarih\"]).sum().reset_index().groupby(by=['musteri']).apply(lin_regression).reset_index().rename(columns={0:\"aylik_toplam_tutar_SUM_slope\"})\n",
        "\n",
        "\n",
        "#amount of each month aggreated then transposed \n",
        "musteri_tarih_level= df_exp.groupby([\"musteri\",\"tarih\"]).agg({'aylik_toplam_tutar': 'sum'})\n",
        "musteri_level= df_exp.groupby(['musteri']).agg({'aylik_toplam_tutar': 'sum'})\n",
        "musteri_tarih_level=musteri_tarih_level.div(musteri_level, level='musteri').multiply(100).reset_index().rename(columns={\"aylik_toplam_tutar\":\"att_perc\"})\n",
        "musteri_tarih_level_tekil=musteri_tarih_level.pivot(index='musteri', columns='tarih', values=\"att_perc\").reset_index()\n",
        "musteri_tarih_level_tekil.columns=[x+\"_att_ratio\" if str(x)[0].isupper() else x for x in musteri_tarih_level_tekil.columns ]\n",
        "\n",
        "#transaction sector counts\n",
        "sector_counts=df_exp.groupby([\"musteri\",\"tarih\"])[\"sektor\"].count().groupby(\"musteri\").max()\n",
        "\n",
        "\n",
        "agg_vars_musteri_level= pd.merge(agg_vars_musteri_level,harcama_trend_tekil,how=\"outer\",on=\"musteri\")\n",
        "agg_vars_musteri_level= pd.merge(agg_vars_musteri_level,musteri_tarih_level_tekil,how=\"outer\",on=\"musteri\")\n",
        "agg_vars_musteri_level= pd.merge(agg_vars_musteri_level,sector_counts,how=\"outer\",on=\"musteri\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiwmZcug-wQv"
      },
      "source": [
        "3) Aggregate Variables for each customer (sector breakdown)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AL74yvEmiIQ"
      },
      "source": [
        "agg_vars_sektor_level=df_exp.groupby(['musteri','sektor']).agg({'islem_adedi':['mean','std','sum','max'],\n",
        "                                                           'aylik_toplam_tutar':['mean','std','sum','max','count'],\n",
        "                                                           'is_outlier_transaction':['mean','sum'],\n",
        "                                                   'tarih':['count']}).reset_index()\n",
        "agg_vars_sektor_level.columns = [\"_\".join(x) if len(x[1]) > 1 else x[0] for x in agg_vars_sektor_level.columns.ravel()]\n",
        "agg_vars_sektor_level_cols=[x  for x in agg_vars_sektor_level.columns if \"_\" in x]\n",
        "agg_vars_sektor_level_tekil=agg_vars_sektor_level.pivot(index='musteri', columns='sektor', values=agg_vars_sektor_level_cols).reset_index()\n",
        "agg_vars_sektor_level_tekil.columns = [\"_\".join(x) if len(x[1]) > 1 else x[0] for x in agg_vars_sektor_level_tekil.columns.ravel()]\n",
        "\n",
        "\n",
        "sektor_trends=df_exp.groupby(by=['musteri','sektor']).apply(lin_regression).reset_index().rename(columns={0:\"slope\"})\n",
        "sektor_trends_tekil=sektor_trends.pivot(index='musteri', columns='sektor', values='slope').reset_index()\n",
        "sektor_trends_tekil.columns = [col + \"_slope\" if col.isupper() else col for col in sektor_trends_tekil.columns] \n",
        "#adding sector-level trend analysis to other sector-level features\n",
        "agg_vars_sektor_level_tekil=pd.merge(sektor_trends_tekil,agg_vars_sektor_level_tekil,how=\"outer\",on=\"musteri\")\n",
        "\n",
        "\n",
        "#ratio of sector / total expenditures\n",
        "sector_ratios=agg_vars_sektor_level_tekil.filter(like=\"aylik_toplam_tutar_sum\").div(agg_vars_musteri_level.aylik_toplam_tutar_sum, axis=0)\n",
        "sector_ratios.columns = [col + \"_ratio\" for col in sector_ratios.columns] \n",
        "agg_vars_sektor_level_tekil=pd.concat([agg_vars_sektor_level_tekil,sector_ratios],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o-2Q0zM_HFP"
      },
      "source": [
        "4) Finding mostly spend sector for eaqch customer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYOFddlNRoYk"
      },
      "source": [
        "\n",
        "max_spent_transaction=df_exp.sort_values(by=[\"musteri\",\"aylik_toplam_tutar\"],ascending=False)\\\n",
        "        .drop_duplicates(subset=\"musteri\",keep=\"first\").drop(columns=[\"islem_adedi\"])\\\n",
        "        .rename(columns={\"sektor\":\"MST_sektor_name\",\"aylik_toplam_tutar\":\"MST_sektor_att\",\"tarih\":\"MST_sektor_month\"})\n",
        "\n",
        "\n",
        "max_spent_ever=df_exp.groupby([\"musteri\",\"sektor\"]).sum().reset_index().sort_values(by=[\"musteri\",\"aylik_toplam_tutar\"]).drop_duplicates(subset=\"musteri\",keep=\"last\")\\\n",
        "                .drop(columns=[\"islem_adedi\"])\\\n",
        "                .rename(columns={\"sektor\":\"MSE_sektor_name\",\"aylik_toplam_tutar\":\"MSE_sektor_att\",\"tarih\":\"MSE_sektor_month\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1agNcJH_q6v"
      },
      "source": [
        "5) Getting the largest difference in consecutive months (assuming spikes may indicate wedding etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFhK-EG44L0U"
      },
      "source": [
        "df_musteri_tarih_total=df_exp.groupby(by=['musteri','tarih']).sum().reset_index()\n",
        "df_musteri_tarih_total[\"difference\"]= df_musteri_tarih_total.sort_values(by=['musteri', 'tarih']).groupby(by=['musteri'])['aylik_toplam_tutar'].diff(periods=1)\n",
        "df_delta= df_musteri_tarih_total.groupby(\"musteri\").agg({'difference':['max','min']}).reset_index()\n",
        "df_delta.columns = [\"_\".join(x) if len(x[1]) > 1 else x[0] for x in df_delta.columns.ravel()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBxoHRS3Ayfp"
      },
      "source": [
        "expenditure_features=pd.merge(agg_vars_sektor_level_tekil,agg_vars_musteri_level,how=\"outer\",on=\"musteri\")\n",
        "expenditure_features=pd.merge(expenditure_features,max_spent_transaction,how=\"outer\",on=\"musteri\")\n",
        "expenditure_features=pd.merge(expenditure_features,max_spent_ever,how=\"outer\",on=\"musteri\")\n",
        "expenditure_features=pd.merge(expenditure_features,df_delta,how=\"outer\",on=\"musteri\")\n",
        "df_merged=pd.merge(df_train,expenditure_features,how=\"inner\",on=\"musteri\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zp5JfoEA-0P"
      },
      "source": [
        "#Feature Elimination\n",
        "\n",
        "-removing highly correlated features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQybxfbmA93b"
      },
      "source": [
        "# Create correlation matrix\n",
        "corr_matrix = df_merged.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find features with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "\n",
        "# Drop features \n",
        "df_merged.drop(to_drop, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JY13YT39qxD"
      },
      "source": [
        "\n",
        "X=df_merged.drop(columns=\"target\").drop(columns=\"musteri\").fillna(0)#[important_features]\n",
        "\n",
        "categorical_features_indices = np.where(X.dtypes== np.object )[0]\n",
        "categorical_features= X.columns[categorical_features_indices]\n",
        "numerical_features= list(set(X.columns) - set(categorical_features))\n",
        "\n",
        "X_with_dummies= pd.get_dummies(X,columns=categorical_features)\n",
        "\n",
        "y=df_merged.target\n",
        "\n",
        "\n",
        "X[categorical_features]=X[categorical_features].fillna(\"nan\") #catboost'a girmeden önce categoricallardaki tüm NaN değerler string'e çevrilmelidir.\n",
        "for f in categorical_features:\n",
        "  X[f]=X[f].astype(str)\n",
        "\n",
        "for feature in categorical_features:\n",
        "  X[feature] = pd.Series(X[feature], dtype=\"category\")\n",
        "\n",
        "categorical_features_indices = np.where(X.dtypes== \"category\" )[0]\n",
        "categorical_features= X.columns[categorical_features_indices]\n",
        "numerical_features= list(set(X.columns) - set(categorical_features))\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKMZvcf4TKy2"
      },
      "source": [
        "# LightGBM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jiQ0wR4jvaY"
      },
      "source": [
        "\n",
        "import lightgbm as lgb\n",
        "Xd= pd.get_dummies(X,columns=categorical_features)\n",
        "Xd_train, Xd_test, yd_train, yd_test = train_test_split(Xd, y, test_size=0.15, random_state=4,stratify=y)\n",
        "Xd_train, Xd_val, yd_train, yd_val = train_test_split(Xd_train, yd_train, test_size=0.2, random_state=4,stratify=yd_train)\n",
        "lightmodel = lgb.LGBMClassifier(objective = \"binary\",class_weight=\"balanced\")\n",
        "lightmodel.fit(  X = Xd_train,  y = yd_train,    eval_set=(Xd_val, yd_val),categorical_feature = 'auto',verbose=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "light_predictions_test=lightmodel.predict(Xd_test)\n",
        "\n",
        "light_acc_test = accuracy_score(yd_test, light_predictions_test)\n",
        "\n",
        "print(\"Lightgbm Accuracy:\"+str(light_acc_test))\n",
        "\n",
        "print(\"Confusion Matrix : \\n\", confusion_matrix(yd_test, light_predictions_test))\n",
        "print(classification_report(yd_test, light_predictions_test, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4pwCWTrT5e_"
      },
      "source": [
        "lgb.plot_importance(lightmodel,max_num_features=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0hL36UuC9Hj",
        "outputId": "f4df4460-8247-481f-c1a7-cb270d18faa8"
      },
      "source": [
        " #Since catboost can work with categorical values without OHE , we proceed so\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=4,stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=4,stratify=y_train)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"x_train y_train shape{X_train.shape}{y_train.shape}\")\n",
        "print(f\"x_val y_val shape{X_val.shape}{y_val.shape}\")\n",
        "print(f\"x_test y_test shape{X_test.shape}{y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train y_train shape(40800, 214)(40800,)\n",
            "x_val y_val shape(10200, 214)(10200,)\n",
            "x_test y_test shape(9000, 214)(9000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnzwbiGyapsq"
      },
      "source": [
        "#Oversampling with SMOTE\n",
        "-Original Ratio: 1:39\n",
        "-Oversampled: 1:1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI74X7auZF8m",
        "outputId": "18e21f17-3dfe-4495-89b9-5d812388f241"
      },
      "source": [
        "from collections import Counter\n",
        "from numpy.random import RandomState\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "print('Original dataset shape (%s, %s)' % X_train.shape)\n",
        "\n",
        "print('Original dataset samples per class {}'.format(Counter(y_train)))\n",
        "\n",
        "\n",
        "sm = SMOTENC(random_state=42, categorical_features=categorical_features_indices,k_neighbors=9)\n",
        "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
        "print('Resampled dataset samples per class {}'.format(Counter(y_res)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape (40800, 147)\n",
            "Original dataset samples per class Counter({0: 39141, 1: 1659})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled dataset samples per class Counter({0: 39141, 1: 39141})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt0e_TZ2zOz5"
      },
      "source": [
        "# CatBoost model with validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyr9wyVpG-pu"
      },
      "source": [
        "catmodel = CatBoostClassifier(\n",
        "    random_seed=42,\n",
        "    logging_level='Verbose',\n",
        "    iterations=200,\n",
        "    use_best_model=True,\n",
        "    class_weights=[1,10],\n",
        "    l2_leaf_reg=5,\n",
        "    \n",
        "    )\n",
        "\n",
        "\n",
        "catmodel.fit(\n",
        "    X_train, y_train,\n",
        "    cat_features=categorical_features_indices,\n",
        "    eval_set=(X_val, y_val),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86bHFVaHGNPw"
      },
      "source": [
        "from sklearn.metrics import fbeta_score,confusion_matrix,accuracy_score,f1_score,classification_report\n",
        "\n",
        "cat_predictions_test=catmodel.predict(X_test)\n",
        "\n",
        "cat_acc_test = accuracy_score(y_test, cat_predictions_test)\n",
        "\n",
        "print(\"Catboost Accuracy:\"+str(cat_acc_test))\n",
        "\n",
        "print(\"Confusion Matrix : \\n\", confusion_matrix(y_test, cat_predictions_test))\n",
        "print(classification_report(y_test, cat_predictions_test, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-kiOL5WY_HM"
      },
      "source": [
        "from catboost.utils import get_roc_curve\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "\n",
        "eval_pool = Pool(X_test, y_test, cat_features=categorical_features_indices)\n",
        "curve = get_roc_curve(catmodel, eval_pool)\n",
        "(fpr, tpr, thresholds) = curve\n",
        "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
        "\n",
        "metrics = catmodel.eval_metrics(\n",
        "    data=eval_pool,\n",
        "    metrics=['Accuracy',\"Precision\",\"Recall\"],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtR5f-5thaVJ"
      },
      "source": [
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "metrics.roc_auc_score(y_test, cat_predictions_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53jnuEvPQoM6"
      },
      "source": [
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, cat_predictions_test)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsPEZhrrbLE6"
      },
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "lw = 2\n",
        "\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc, alpha=0.5)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.grid(True)\n",
        "plt.xlabel('False Positive Rate', fontsize=16)\n",
        "plt.ylabel('True Positive Rate', fontsize=16)\n",
        "plt.title('Receiver operating characteristic', fontsize=20)\n",
        "plt.legend(loc=\"lower right\", fontsize=16)\n",
        "\n",
        "\n",
        "from catboost.utils import get_fpr_curve\n",
        "from catboost.utils import get_fnr_curve\n",
        "\n",
        "(thresholds, fpr) = get_fpr_curve(curve=curve)\n",
        "(thresholds, fnr) = get_fnr_curve(curve=curve)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "lw = 2\n",
        "\n",
        "plt.plot(thresholds, fpr, color='blue', lw=lw, label='FPR', alpha=0.5)\n",
        "plt.plot(thresholds, fnr, color='green', lw=lw, label='FNR', alpha=0.5)\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.grid(True)\n",
        "plt.xlabel('Threshold', fontsize=16)\n",
        "plt.ylabel('Error Rate', fontsize=16)\n",
        "plt.title('FPR-FNR curves', fontsize=20)\n",
        "plt.legend(loc=\"lower left\", fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utGIVEPYdl99"
      },
      "source": [
        "test_results=pd.concat([pd.concat([df_merged.iloc[X_test.index].musteri,X_test,y_test,],axis=1).reset_index(drop=True),pd.Series(cat_predictions_test,name=\"prediction\")],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEXkDu03bIKY"
      },
      "source": [
        "missed= test_results[[\"musteri\",\"yas\",\"kidem_suresi\",\"egitim\",\"is_durumu\",\"meslek_grubu\",\"target\",\"prediction\"]].query(\"target==1 and prediction==0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U27b7sfSCDce"
      },
      "source": [
        "df_submission=pd.DataFrame(data={\"musteri\":df_test.musteri,\"target\":catmodel.predict(df_test_merged)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlLyxLb5CYC7"
      },
      "source": [
        "df_submission.to_csv(\"13_02_2021_catmodel_v2.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvUIJ2eVBmHO"
      },
      "source": [
        "catmodel.predict(df_test_merged)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}